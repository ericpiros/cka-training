# High Availability for Kubernetes
There are 2 common ways for you to enable high availability for your Kubernetes cluster:
* **Stacked ETCD** - where your ETCD installation is stacked on top of the other control plane components. This is the default that Kubeadm will setup unless you specify a separate ETCD server. It is worth noting that in this setup, the kube-apiserver on a host will only communicate with the ETCD server on the same hosts. With this setup it is recommended that you have atleast 3 servers. This is to ensure that your ETCD cluster can elect a leader in case a node goes down. The down side of this setup is that if a node goes down, you lose a control plane and ETCD instance.
* **External ETCD** - as the name describes, this is when your ETCD cluster is separate from the control plane components. The advantage of this setup is that losing a node limits the impact of the loss (i.e. if you lose a node with the api-server, you only lose 1 instance of an api-server). The downside to this is that you will need more servers for this setup.  

On both topologies, you will need to have a load balancer in front of your kube-apiserver. Both topologies are explained well in [this](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/) page of the Kubernetes documentation.  


# Setting up a Highly Available Stacked Cluster using Kubeadm 
I have created a HA version of the Vagrant single Control Plane vagrantfile [here](../vagrant/ha_cluster_virtualbox/Vagrantfile). This will bring up the following: 
* Jumpbox - a server we can use to connect to the other servers. Also once a Kubernetes user is created, this is where I mostly execute my commands on. 
* Nginx - this will serve as our load balancer.
* Master1-3 - these will be our Control Plane nodes.
* Node1-2 -these will be our worker nodes.  

This version of the vagrant file will also install the pre-required software and settings. It will also add entries to the '/etc/hosts' file on each server to help with name resolution.  

If you are using libvirt, it will be best tobring up the VMs 1 at a time so as not to saturate your network interface (especially if you are running this on a laptop like me and do not have access to 10GB ethernet). This can be done by doing 'vagrant up < VM name >'.  
```
$ vagrant up master1

$ vagrant up master2

$ vagrant up proxy

$ vagrant up node1
```  

Once all the servers are up, we will need to connect to **master1** first and setup Kubernetes using kubeadm. The process will be pretty much the same with only 1 difference. We will set the '--copntrol-plane-endpoint' parameter to either the IP address or name of the Nginx server.  
```
# On master1
$ sudo kubeadm init --apiserver-advertise-address=192.168.56.5 --control-plane-endpoint=192.168.56.4:6443 --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.26.0 --upload-certs
```  

The '--upload-certs' parameter is also important, as we will need make the certificates generated by kubeadm available to all 3 Control Plane nodes. An alternative to this would be to generate the certificates ourselves and place it somewhere the 3 Control Plane nodes can access it. Take note that the uploaded certs will only be available for 2 hours. You can re-upload the certificates and generate a new decryption key using the following commands:  
```
# Generate new certificates and upload them.
$ sudo kubeadm init phase upload-certs --upload-certs
```  

Alternatively I have also created a config file for this located in the ['/vagrant'](../../vagrant/ha_cluster_virtualbox/my-k8s-cluster.yaml) synched folder:
```
# Use a config file.
$ sudo kubeadm init --config /vagrant/my-k8s-cluster.yaml --upload-certs
<-- Output truncated -->
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.56.4:6443 --token <token> \
	--discovery-token-ca-cert-hash <cert hash> \
	--control-plane --certificate-key <cert decryption key>

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.56.4:6443 --token <token> \
	--discovery-token-ca-cert-hash <cert hash> 
```  

Now as can be seen in the output above, the join command to add another controplan node is given in the output. The main difference between the workder node join command is that the following 2 parameters are added:  
* **--control-plane** - tells kubeadm that the node we are joining will be a control plane node.
* **--certificate-key** - this is the decryption key for the certificates uploaded to secrets by the init command.  
* **--apiserver-advertise-address** - this is only required on our setup where we have 2 network cards on our servers. Set this to the IP address of the node that you are going to add to the control plane.

Lets add master2 to the cluster as our second Control Plane node. I have already gone ahead and joined node1 and node2 in this example.  

## Note:
Since our setup uses 2 network interface cards, we will need to add the '--apiserver-advertise-address' on our join commands for new control plane nodes so that kubeadm does not use the default network card.
```
# On master1
# Lets list our current nodes.
$ kubectl get nodes
NAME      STATUS     ROLES           AGE     VERSION
master1   NotReady   control-plane   10m     v1.26.0
node1     NotReady   <none>          3m41s   v1.26.0
node2     NotReady   <none>          2m46s   v1.26.0

# On master2.
# Join the server to the cluster.
$ sudo kubeadm join 192.168.56.4:6443 --token <cert token> \
	--discovery-token-ca-cert-hash <cert hash> \
	--control-plane --certificate-key <cert decryption key> \
	--apiserver-advertise-address 192.168.56.6

<-- Output truncated -->
This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

# On master1.
# Lets check our nodes now.
$ kubectl get nodes
NAME      STATUS     ROLES           AGE    VERSION
master1   NotReady   control-plane   22m    v1.26.0
master2   NotReady   control-plane   5m9s   v1.26.0
node1     NotReady   <none>          16m    v1.26.0
node2     NotReady   <none>          15m    v1.26.0
```  

As can be seen above we have joined master2 as a control-plane node. The nodes are in a not-ready state as we have not installed any CNI yet. Lets first join master3 as our last Control Plane node. For this, lets pretend that we have spun up master3 several days later. As such the certificates we have uploaded are no longer in the cluster's secrets. To join the new server, we will need to first know the join command. As with the simple cluster installation we did in the first section, we can use the 'kubeadm token create' command to generate a new token and show use the join command.  
```
# Generate a new token and show the join command.
# On master1.
$ sudo kubeadm token create --print-join-command
kubeadm join 192.168.56.4:6443 --token <new token> --discovery-token-ca-cert-hash <cert hash>
```  

Now we will need to create new certificates and upload them again.  
```
# Generate new certification, decryption key and upload them to the sercrets.
# On master1.
$ sudo kubeadm init phase upload-certs --upload-certs
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
<cert decryption key>
```  

Now we can assemble the commands needed to join master3.  
```
# Join master3 as a control plane node.
# On master3.
$ sudo kubeadm join 192.168.56.4:6443 --token <new token> \
	--discovery-token-ca-cert-hash <cert hash> \
    --control-plane --certificate-key <cert decryption key> \
	--apiserver-advertise-address 192.168.56.7 

<-- Output truncated -->
This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

# On master1
NAME      STATUS   ROLES           AGE   VERSION
master1   Ready    control-plane   34h   v1.26.0
master2   Ready    control-plane   34h   v1.26.0
master3   Ready    control-plane   34h   v1.26.0
node1     Ready    <none>          34h   v1.26.0
node2     Ready    <none>          34h   v1.26.0
```  

We now have 3 control plane nodes in our cluster. If the status of your nodes **NotReady** then this means that you have not installed a CNI plugin yet. Choose 1 and install it on your cluster.  

We can also confirm that we have a HA cluster by checking the members of our ETCD. We can do this by running 'etcdctl member list'.
```
# View ETCD members
# Run this on master1
$ sudo ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
13240cf8591c5ceb, started, master1, https://192.168.73.5:2380, https://192.168.73.5:2379
780e972a423692e1, started, master3, https://192.168.73.7:2380, https://192.168.73.7:2379
784b48a9d0a745bb, started, master2, https://192.168.73.6:2380, https://192.168.73.6:2379
```  
As seen above we confirm that our ETCD cluster has all our master nodes as its members.
